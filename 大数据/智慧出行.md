# 一、搭建项目zhcx

pom

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.aishang</groupId>
    <artifactId>data_producer</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients -->
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>0.11.0.2</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/com.alibaba/fastjson -->
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.41</version>
        </dependency>

    </dependencies>


</project>
```

# 二、生产数据到kafka

kafka.properties

```properties
#设置生产者
bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 
key.serializer=org.apache.kafka.common.serialization.StringSerializer
value.serializer=org.apache.kafka.common.serialization.StringSerializer
#数据安全级别
acks=all
#数据发送失败  重试为0
reties=0

#设置主题
kafka.topics=zhcx



#消费者
group.id=zhcx1
#设置offset，每个30秒设置偏移量
enable.atuo.commit=true
auto.commit.interval.ms=30000


#设置follower和leader的同步时间
zookeeper.sync.time.ms=250
num.io.threads=12
batch.size=65536
buffer.memory=524288
#保存时间，默认168
log.retention.hours=5


```

读取配置文件的工具类

```scala
package util

import java.util.Properties

object PropertiesUtil {
  val properties = new Properties
  try {
    val inputStream = ClassLoader.getSystemResourceAsStream("kafka.properties")
    properties.load(inputStream)
  } catch {
    case e: Exception => e.getStackTrace
  } finally {

  }

  def getProperty(key:String):String=properties.getProperty(key) 
}

```

# 三、创建生产者

```scala
package main

import java.lang.Thread
import java.text.DecimalFormat
import java.util.Calendar

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}
import util.PropertiesUtil
import java.util

import com.alibaba.fastjson.JSON

import scala.util.Random

object Producer {
  def main(args: Array[String]): Unit = {
    //    读取kafka配置信息
    val props = PropertiesUtil.properties
    //    创建kafka生产者对象
    val producer = new KafkaProducer[String, String](props)
    //    模拟生产实时数据，每5分钟切换一次车辆速度
    //    单位：秒
    var startTime = Calendar.getInstance().getTimeInMillis / 1000
    //设置5分钟切换速度，实际测试缩短时间即可，现在是5秒
    val changeStatus = 5
    //    开始模拟数据
    while (true) {
      //      模拟产生监测点
      //      格式化数字，1--》00001      模拟20个监测点
      val randomMonitorId = new DecimalFormat("0000").format(Random.nextInt(20) + 1)
      //      模拟堵车切换周期,设置5分钟切换速度
      val currentTime = Calendar.getInstance().getTimeInMillis / 1000
      //设置车速
      var randomSpeed = ""
      if (currentTime - startTime > changeStatus) {
        randomSpeed = new DecimalFormat("000").format(Random.nextInt(16)) //0~15 speed  堵车状态
        if (currentTime - startTime > changeStatus * 2) {
          startTime = currentTime
        }
      } else {
        randomSpeed = new DecimalFormat("000").format(Random.nextInt(31) + 30) //30~60 speed  正常
      }
      //        测试
      //      println("监测点:" + randomMonitorId + ",速度:" + randomSpeed)
      //      将数据转换成json
      val jsonMap = new util.HashMap[String, String]()
      jsonMap.put("randomMonitorId", randomMonitorId)
      jsonMap.put("randomSpeed", randomSpeed)
      //      转成json
      val json = JSON.toJSON(jsonMap)
      println(json)
//      发送信息到kafka
      producer.send(new ProducerRecord[String,String](props.getProperty("kafka.topics"),json.toString))
      Thread.sleep(1000)
      //效果是 第一次速度正常，隔了5秒后 开始堵车
    }

  }
}

```

# 四、启动集群

```sh
[root@hadoop102 config]# vi server.properties 
log.dirs=/opt/module/kafka/logs
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181
zookeeper.connection.timeout.ms=60000


#分发到其他机器
xsync

之后修改其他机器的broker.id    不能重复


#启动前保证hadoop zookeeper开启,启动三台
bin/kafka-server-start.sh config/server.properties 

#创建主题
bin/kafka-topics.sh --create --zookeeper hadoop102:2181 --partitions 2 --replication-factor 2 --topic zhcx
#查看
 bin/kafka-topics.sh --list --zookeeper hadoop102:2181
 
 
 #测试 创建消费者

#新版本命令，存到kafka集群上，提高效率
bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic zhcx --from-beginning
```

运行生产数据的程序

# 五、创建消费者，存到redis

pom

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.aishang</groupId>
    <artifactId>redis_consumer</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients -->
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>0.11.0.2</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/com.alibaba/fastjson -->
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.41</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.3.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka_2.11</artifactId>
            <version>1.6.3</version>
        </dependency>
        <dependency>
            <groupId>redis.clients</groupId>
            <artifactId>jedis</artifactId>
            <version>2.9.0</version>
        </dependency>
    </dependencies>
</project>
```

kafka.properties

```properties
#设置消费者
bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
key.deserializer=org.apache.kafka.common.serialization.StringDeSerializer
value.deserializer=org.apache.kafka.common.serialization.StringDeSerializer
#数据安全级别
acks=all
#数据发送失败  重试为0
reties=0

#设置主题
kafka.topics=zhcx



#消费者
group.id=zhcx1
#设置offset，每个30秒设置偏移量
enable.atuo.commit=true
auto.commit.interval.ms=30000


#设置follower和leader的同步时间
zookeeper.sync.time.ms=250
num.io.threads=12
batch.size=65536
buffer.memory=524288
#保存时间，默认168
log.retention.hours=5



metadata.broker.list=hadoop102:9092,hadoop103:9092,hadoop104:9092


```

工具类复制之前的

```scala
package util

import java.util.Properties

object PropertiesUtil {
  val properties = new Properties
  try {
    val inputStream = ClassLoader.getSystemResourceAsStream("kafka.properties")
    properties.load(inputStream)
  } catch {
    case e: Exception => e.getStackTrace
  } finally {

  }

  def getProperty(key:String):String=properties.getProperty(key)
}

```

Redis工具类

- 启动redis虚拟机

```scala
package util

import redis.clients.jedis.{JedisPool, JedisPoolConfig}

object RedisUtil {
//配置redis连接器
  val host = "192.168.83.120"
  val port = 6379
  val timeout = 30000

  val config = new JedisPoolConfig
  config.setMaxTotal(200)
  config.setMaxIdle(50)
  config.setMinIdle(0)
  config.setMaxWaitMillis(10000)
  config.setTestOnBorrow(true)
  config.setTestOnReturn(true)
  config.setTestOnCreate(true)

  config.setTestWhileIdle(true)
  config.setTimeBetweenEvictionRunsMillis(30000)
  config.setNumTestsPerEvictionRun(10)
  config.setMinEvictableIdleTimeMillis(60000)

//  连接池
  lazy val pool = new JedisPool(config,host,port,timeout)

//  创建程序崩溃时，回收资源的线程
  lazy  val hook = new Thread{
    override def run() = {
      pool.destroy()
    }
  }
  sys.addShutdownHook(hook)

}

```

创建消费者

```scala
import java.text.SimpleDateFormat
import java.util.Calendar

import com.alibaba.fastjson.{JSON, TypeReference}
import kafka.serializer.StringDecoder
import org.apache.spark.SparkConf
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}
import util.{PropertiesUtil, RedisUtil}

object Consumer {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Consumer").setMaster("local[*]")
    val ssc = new StreamingContext(conf, Seconds(5))
    ssc.checkpoint("./ssc/checkpoint")
    //    配置kafka参数
    val kafkaParams = Map("metadata.broker.list" -> PropertiesUtil.getProperty("metadata.broker.list"))
    //想要消费数据在哪个主题
    val topics = Set(PropertiesUtil.getProperty("kafka.topics"))
    //读取kafka中的value数据
    val kafkaLineDStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](
      ssc, kafkaParams, topics
    ).map(_._2)
    //测试从kafka获取数据
    /*
      No output streams registered, so nothing to execute
      解决原因：没有触发DStream需要的aciton
      解决方法：使用以下方法之一触发：
      print()
      foreachRDD()
      saveAsObjectFiles()
      saveAsTextFiles()
      saveAsHadoopFiles()


      -------------------------------------------
      Time: 1567728240000 ms
      -------------------------------------------
      {"randomMonitorId":"0002","randomSpeed":"006"}
      {"randomMonitorId":"0011","randomSpeed":"009"}
      {"randomMonitorId":"0017","randomSpeed":"009"}
      {"randomMonitorId":"0009","randomSpeed":"005"}
      {"randomMonitorId":"0001","randomSpeed":"001"}
     */
    //    kafkaLineDStream.print()

    //解析读取到的数据
    val event = kafkaLineDStream.map(line => {
      val lineJavaMap = JSON.parseObject(line, new TypeReference[java.util.Map[String, String]]() {})
      //将java的map转成scala的map
      import scala.collection.JavaConverters._
      val lineScalaMap: collection.mutable.Map[String, String] = mapAsScalaMapConverter(lineJavaMap).asScala
      println(lineScalaMap)
      lineScalaMap
    })

    //将数据进行简单聚合
    //原数据{"randomMonitorId":"0017","randomSpeed":"009"}
    //目标(0002,(500,10))
    val sumOfSpeedAndCount = event
      .map(e => (e.get("randomMonitorId").get, e.get("randomSpeed").get)) //(0002,35)
      .mapValues(v => (v.toInt, 1)) //(0002,(35,1))
      .reduceByKeyAndWindow((t1: (Int, Int), t2: (Int, Int)) => (t1._1 + t2._1, t1._2 + t2._2), Seconds(20), Seconds(10))
    //将某一时间窗口内的数据进行聚合,第一个Seconds(5)是时间窗口的宽度（每次统计5秒内的数据），第二个Seconds(5)，就是
    //统计完后，还要移动到下一个窗口，移动的宽度也是5秒
    //此时的数据是：(0002,(500,10)) 0002窗口，5秒内总速度是500，总共10台车
    //将此数据存放到redis中
    val redisIndex = 1 //redis数据库索引，我们存放在第一个就可以
    sumOfSpeedAndCount.foreachRDD(rdd => {
      rdd.foreachPartition(p => {
        p.filter((tuple: (String, (Int, Int))) => tuple._2._2 > 0)
          .foreach(pair => {
            val jedis = RedisUtil.pool.getResource
            val randomMonitorId = pair._1
            //0002
            val sumOfSpeed = pair._2._1
            //500
            val sumOfCarCount = pair._2._2 //10

            //将数据实时保存到redis中
            val currentTime = Calendar.getInstance().getTime
            val hmSDF = new SimpleDateFormat("HHmm")
            val dateSDF = new SimpleDateFormat("yyyyMMdd")
            //时间格式化
            val hourMinuteTime = hmSDF.format(currentTime)
            //2035
            val date = dateSDF.format(currentTime) //20190918
            jedis.select(redisIndex)
            //key  ->  0001_20190918
            //fields -> 2035
            //value -> 500_10
            jedis.hset(date + "_" + randomMonitorId, hourMinuteTime, sumOfSpeed + "_" + sumOfCarCount)
            //RedisUtil.pool.returnResource(jedis)
            jedis.close()
          })
      })
    })

    //启动sparkstreaming
    ssc.start()
    ssc.awaitTermination()
  }
}

```

Redis3.0.4

```sh
#配置文件
daemonize yes
bind 192.168.83.102   #当前虚拟机ip


make && make install
#启动
src/redis-server redis.conf 
#测试
src/redis-cli -h 192.168.83.102
192.168.83.170:6379> set hello 123
OK
192.168.83.170:6379> get hello
"123"


flushall
keys *
```

pom

```xml
 <!--redis template-->
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-pool2</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-redis</artifactId>
        </dependency>
```

yml

```yml
 redis:
    host: 192.168.83.102
    port: 6379
    # 密码 没有则可以不填
#    password: 123456
    # 如果使用的jedis 则将lettuce改成jedis即可
    lettuce:
      pool:
        # 最大活跃链接数 默认8
        max-active: 8
        # 最大空闲连接数 默认8
        max-idle: 8
        # 最小空闲连接数 默认0
        min-idle: 0
    database: 1
    #选择数据库1
```

- 读取redis数据

```java
package com.aishang.showinfo.service.impl;

import com.aishang.showinfo.po.ZHCX;
import com.aishang.showinfo.po.ZHCXExt;
import com.aishang.showinfo.service.ZHCXService;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.stereotype.Service;

import javax.annotation.Resource;
import java.util.*;
import java.util.stream.Collectors;

@Service
public class ZHCXServiceImpl implements ZHCXService {
    //注入redis模板对象
    @Resource
    RedisTemplate<String, ?> redisTemplate;

    @Override
    public List<ZHCXExt> getAvgSpeed() {

        Set<String> keys = redisTemplate.keys("*");//20190907_0011
        //遍历所有keys，并获取到监测点
        Map<String, String> addrMap = new HashMap<>();
        addrMap.put("0001", "红旗大街");
        addrMap.put("0002", "三大动力路");
        addrMap.put("0003", "征仪路");
        addrMap.put("0004", "通乡街");
        addrMap.put("0005", "中兴大道");
        addrMap.put("0006", "学府路");
        addrMap.put("0007", "南直路");
        addrMap.put("0008", "先锋路");
        addrMap.put("0009", "哈平路");
        addrMap.put("0010", "和平路");

        Map<String, Map<String, Double>> map = new HashMap<>();
        List<ZHCX> list = new ArrayList<>();
        Map<String, Double> timeAndAvg = new HashMap<>();
        for (String key : keys) {
            //System.out.println("监测点="+key.substring(key.lastIndexOf("_")+1));//0020
            String id = key.substring(key.lastIndexOf("_") + 1);
            if (addrMap.containsKey(id)) {
                Set<Map.Entry<Object, Object>> entries = redisTemplate.opsForHash().entries(key).entrySet();
                for (Map.Entry<Object, Object> entry : entries) {
                    ZHCX zhcx = new ZHCX();
                    zhcx.setId(addrMap.get(id));
                    zhcx.setTime((String) entry.getKey());
                    String[] s = entry.getValue().toString().split("_");
                    int sumSpeed = 0;
                    int carCount = 0;
                    for (int i = 0; i < s.length; i++) {
                        sumSpeed = Integer.parseInt(s[0]);
                        carCount = Integer.parseInt(s[1]);
                    }
                    double avg = (double) sumSpeed / carCount;
                    zhcx.setAvgSpeed(avg);
                    list.add(zhcx);
                }
            }
        }
        List<ZHCXExt> ZHCXExtList = new ArrayList<>();
        //跟据某个属性分组
        Map<String, List<ZHCX>> collect = list.stream().collect(Collectors.groupingBy(ZHCX::getId));
        Set<Map.Entry<String, List<ZHCX>>> entries = collect.entrySet();
        for (Map.Entry<String, List<ZHCX>> entry : entries) {
            ZHCXExt zhcxExt = new ZHCXExt();
            zhcxExt.setId(entry.getKey());
            //取出一组对象的某个属性组成一个新集合
            List<String> times = entry.getValue().stream().map(ZHCX::getTime).collect(Collectors.toList());
            List<Double> avgs = entry.getValue().stream().map(ZHCX::getAvgSpeed).collect(Collectors.toList());
            zhcxExt.setTimes(times);
            zhcxExt.setAvgs(avgs);
            ZHCXExtList.add(zhcxExt);
            //[ZHCXExt(
            // id=三大动力路, times=[0918, 0919, 0920, 0921, 0922, 0923, 0924],
            // avgs=[17.6, 21.0, 33.0, 31.333333333333332, 3.3333333333333335, 10.0, 43.0]),
            // ZHCXExt(id=和平路, times=[0918, 0919, 0920, 0921, 0922, 0923, 0924],
            // avgs=[18.0, 24.333333333333332, 27.2, 26.0, 5.0, 26.2, 35.8]),
        }
        return ZHCXExtList;
    }


}

```

# 六、建模

pom

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.aishang</groupId>
    <artifactId>Wisdom_travel_modeling</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <!--redis template-->
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-pool2</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-redis</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>redis.clients</groupId>
            <artifactId>jedis</artifactId>
            <version>2.9.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
    </dependencies>
</project>
```

复制之前的util包