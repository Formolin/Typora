# Movie推荐系统

![image-20190818160035583](http://ww1.sinaimg.cn/large/006tNc79ly1g63w28ymh6j31ii0u0awl.jpg)

## 一、安装MongoDB

```sh
cd /opt/module
wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.4.3.tgz
#解压
#改名
 mv mongodb-linux-x86_64-rhel62-3.4.3/ mongodb
 cd mongodb/
 
#创建data文件夹用于存放数据和日志
mkdir data
cd data
mkdir db
mkdir logs

cd logs
touch mongodb.log
#data目录下创建mongodb.conf
touch mongodb.conf
```

mongodb.conf

```sh
#端口号
port = 27017
#数据目录
dbpath = /opt/module/mongodb/data/db
#日志目录
logpath = /opt/module/mongodb/data/logs/mongodb.log
#设置后台运行
fork = true
#日志输出方式
logappend = true
#开启认证
#auth = true
```

- 启动

```sh
bin/mongod -config /opt/module/mongodb/data/mongodb.conf

#about to fork child process, waiting until server is ready for connections.
#forked process: 57347
#child process started successfully, parent exiting

```

- 打开robo连接测试

![image-20190818133422326](http://ww4.sinaimg.cn/large/006tNc79ly1g63ru44293j30l00g2abi.jpg)

- 关闭

```sh
bin/mongod -shutdown -config /opt/module/mongodb/data/mongodb.conf
```

## 二、安装redis

```sh
#上传、解压
cd redis
#安装GCC 
sudo yum install gcc
#编译源代码
make MALLOC=libc
#编译安装
sudo make install

#修改配置文件中以下内容  ?daemonize  查找
daemonize yes
bind 0.0.0.0    #64行  #绑定主机IP，默认值为127.0.0.1，我们是跨机器运行，所以需要更改
logfile "/opt/module/redis-logs/redis.log"   #104行  #定义log文件位置，模式log信息定向到stdout，输出到/dev/null（可选）
#在moudle下创建文件夹 redis-logs，上面路径改为此路径

dir "/opt/module/redis-logs" #188行  #本地数据库存放路径，默认为./，编译安装默认存在在/usr/local/bin下（可选）

#创建配置文件,默认安装到/usr/local/bin下
sudo cp redis.conf ../redis-logs/

#启动 cd到module
redis-server redis-logs/redis.conf

#由于是后台启动，查看，cd到redis-logs,cat redis.log
# Ready to accept connections

#测试
[root@movie redis-logs]# redis-cli 
127.0.0.1:6379> set a b
OK
127.0.0.1:6379> get a
"b"
127.0.0.1:6379> 
exit

#停止
redis-cli shutdown
```

## 三、ElasticSearch（单节点）环境配置

```sh
#上传解压
#修改linux配置参数
sudo vi /etc/security/limits.conf
#修改文件数配置，在文件末尾添加如下配置
* soft nofile 65536
* hard nofile 131072
* soft nproc 2048
* hard nproc 4096
#修改* soft nproc 1024 为 * soft nproc 2048
cd cd /etc/security/limits.d/
sudo vi 20-nproc.conf
* soft nproc 2048     #将该条目修改成2048

sudo vi /etc/sysctl.conf
# 在文件末尾添加：
vm.max_map_count=655360

sudo sysctl -p
```

配置ElasticSearch

```sh
cd elasticsearch-5.6.2/
mkdir elasticsearch-5.6.2/data/
mkdir elasticsearch-5.6.2/logs/

vi config/elasticsearch.yml

cluster.name: es-cluster  #设置集群的名称
node.name: es-node   #修改当前节点的名称
path.data: /opt/module/elasticsearch/data  #修改数据路径
path.logs: /opt/module/elasticsearch/logs  #修改日志路径
bootstrap.memory_lock: false   #设置ES节点允许内存交换
bootstrap.system_call_filter: false   #禁用系统调用过滤器
network.host: 192.168.83.30    #设置当前主机名称或者修改hosts  movie
discovery.zen.ping.unicast.hosts: ["192.168.83.30"]   #设置集群的主机列表

#启动ElasticSearch服务
bin/elasticsearch -d
#不能使用root账户启动
chmod 777 elasticsearch
当前用户没有执行权限 
解决方法： chown linux用户名 elasticsearch安装目录 -R 
chown  admin elasticsearch -R

adduser admin
su admin
bin/elasticsearch -d

#jps查看进程
http://192.168.83.30:9200/

#停止ElasticSearch服务
jps
kill -9 8514
```

开始安装head

```sh
#进入安装目录下的config目录，修改elasticsearch.yml文件.在文件的末尾加入以下代码
http.cors.enabled: true 
http.cors.allow-origin: "*"
node.master: true
node.data: true
#修改
network.host: 0.0.0.0

#启动es

在https://github.com/mobz/elasticsearch-head中下载head插件，选择下载zip


解压到指定文件夹下，G:\elasticsearch-6.6.2\elasticsearch-head-master  进入该文件夹，修改G:\elasticsearch-6.6.2\elasticsearch-head-master\Gruntfile.js 在对应的位置加上hostname:'*'

connect: {
			server: {
				options: {
					hostname:'*',
					port: 9100,
					base: '.',
					keepalive: true
				}
			}
		}
```

![img](http://ww2.sinaimg.cn/large/006tNc79ly1g63trqhx5pj30b905aa9v.jpg)

在G:\elasticsearch-6.6.2\elasticsearch-head-master  下执行npm install 安装完成后执行grunt server 或者npm run start 运行head插件，如果不成功重新安装grunt。成功如下

## 四、Azkaban（单节点）环境配置

```sh
yum install git
git clone https://github.com/azkaban/azkaban.git
cd azkaban/
#切换到3.36.0版本
git checkout -b 3.36.0
#安装编译环境
yum install gcc
 yum install -y gcc-c++
./gradlew clean build
```

部署Azkaban Solo

```sh
#将编译好的azkaban中的azkaban-solo-server-3.36.0.tar.gz拷贝到根目录
#启动Azkaban Solo单节点服务
cd azkaban-solo-server-3.36.0/
#修改端口号，spark使用了8081，az默认也是8081
jetty.port=8085

bin/azkaban-solo-start.sh
#程序显示运行中，直接回车变后台运行

http://192.168.83.30:8085/
azkaban
azkaban

#关闭
 bin/azkaban-solo-shutdown.sh
```



## 五、安装spark

```sh
cd spark/conf
cp slaves.template slaves
vi slaves
#在文件最后将本机主机名进行添加
movie


cp spark-env.sh.template spark-env.sh
vi spark-env.sh

SPARK_MASTER_HOST=movie
SPARK_MASTER_PORT=7077
#启动Spark

sbin/start-all.sh
#启动报错java no set

#sbin目录下的spark-config.sh 文件中加入如下配置：
export JAVA_HOME=/opt/module/jdk1.8

jps查看
有worker、master


http://192.168.83.30:8080/

#停止
sbin/stop-all.sh
```

## 六、安装zookeeper(单节点)

```sh
#进入zookeeper安装目录
 cd zookeeper
#创建data数据目录
mkdir data/
#复制zookeeper配置文件
cd conf
cp zoo_sample.cfg zoo.cfg   
#修改zookeeper配置文件
vi zoo.cfg

dataDir=/opt/module/zookeeper/data  #将数据目录地址修改为创建的目录
#启动Zookeeper服务
bin/zkServer.sh start
#查看Zookeeper服务状态
bin/zkServer.sh status

ZooKeeper JMX enabled by default
Using config: /home/bigdata/cluster/zookeeper-3.4.10/bin/../conf/zoo.cfg
Mode: standalone
#关闭Zookeeper服务
bin/zkServer.sh stop
```

## 七、Kafka（单节点）环境配置

```sh
cd kafka
vi config/server.properties
host.name=movie                  #修改主机名
port=9092                         #修改服务端口号
zookeeper.connect=movie:2181     #修改Zookeeper服务器地址
#启动kafka服务 !!! 启动之前需要启动Zookeeper服务
bin/kafka-server-start.sh -daemon ./config/server.properties
#关闭kafka服务
bin/kafka-server-stop.sh
#测试
#创建topic
bin/kafka-topics.sh --create --zookeeper movie:2181 --replication-factor 1 --partitions 1 --topic recommender
#kafka-console-producer  一个生产者窗口
bin/kafka-console-producer.sh --broker-list movie:9092 --topic recommender
#kafka-console-consumer 一个消费者窗口
bin/kafka-console-consumer.sh --bootstrap-server movie:9092 --topic recommender

#输入内容，生成消费
```

## 八、Flume-ng（单节点）环境配置

```sh
#先上传解压，后续再用
```

## 九、Apache环境配置

```sh
#安装HTTPD
 yum install httpd
#启动HTTPD
service httpd start
#关闭HTTPD
sudo service httpd stop
#访问Apache服务器,通过浏览器访问http://192.168.83.30:80
```

## 十、Tomcat环境配置

```sh
#上传解压,启动tomcat服务
bin/startup.sh
#停止
bin/shutdown.sh

http://192.168.83.30:8080
此时与spark冲突，停止spark，先启动tomcat，spark发现8080被占用，会往后排

#查看日志
 tail -f logs/catalina.out
```

```sh
jps


76736 Master    #spark
76899 Worker    #spark
78004 AzkabanSingleServer   #Azkaban
51429 Elasticsearch
76295 Bootstrap  #tomcat
78264 Jps
110523 QuorumPeerMain #zookeeper
1386 GradleDaemon 
58878 Kafka
```

## 十一、创建项目

- 创建maven项目movieSystem
- 删除src
- 修改pom文件

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.aishang</groupId>
    <artifactId>movieSystem</artifactId>
    <version>1.0-SNAPSHOT</version>
    <packaging>pom</packaging>


</project>
```

- 新建moudle

  - recommender
  - 在把这个项目当成父项目，修改pom

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  <project xmlns="http://maven.apache.org/POM/4.0.0"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
      <parent>
          <artifactId>movieSystem</artifactId>
          <groupId>com.aishang</groupId>
          <version>1.0-SNAPSHOT</version>
      </parent>
      <modelVersion>4.0.0</modelVersion>
  
      <artifactId>recommender</artifactId>
      <packaging>pom</packaging>
  
  
  </project>
  ```

  

- 删掉src

- 在recommender下新建moudle叫dataloader

  - 此时注意路径问题！！！！

  ![image-20190818165115893](/Users/liujiang/Documents/Typora/imgs/006tNc79ly1g63xiydvmtj31d2082myk.jpg)

### MovieSystem的pom文件

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.aishang</groupId>
    <artifactId>movieSystem</artifactId>
    <version>1.0-SNAPSHOT</version>
    <modules>
        <module>recommender</module>
    </modules>
    <packaging>pom</packaging>

    <!-- 声明子项目公用的配置属性 -->
    <properties>
        <spark.version>2.1.1</spark.version>
        <scala.version>2.11.8</scala.version>
        <log4j.version>1.2.17</log4j.version>
        <slf4j.version>1.7.22</slf4j.version>
    </properties>

    <!-- 声明并引入子项目共有的依赖 -->
    <dependencies>

        <!-- 所有子项目的日志框架 -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>jcl-over-slf4j</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        <!-- 具体的日志实现 -->
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
            <version>${log4j.version}</version>
        </dependency>
        <!-- Logging End -->

    </dependencies>

    <!-- 仅声明子项目共有的依赖，如果子项目需要此依赖，那么子项目需要声明 -->
    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.scala-lang</groupId>
                <artifactId>scala-library</artifactId>
                <version>${scala.version}</version>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <!-- 声明构建信息 -->
    <build>
        <!-- 声明并引入子项目共有的插件【插件就是附着到Maven各个声明周期的具体实现】 -->
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.6.1</version>
                <!-- 所有的编译都依照JDK1.8来搞 -->
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
        </plugins>

        <!-- 仅声明子项目共有的插件，如果子项目需要此插件，那么子项目需要声明 -->
        <pluginManagement>
            <plugins>
                <!-- 该插件用于将Scala代码编译成class文件 -->
                <plugin>
                    <groupId>net.alchim31.maven</groupId>
                    <artifactId>scala-maven-plugin</artifactId>
                    <version>3.2.2</version>
                    <executions>
                        <execution>
                            <!-- 声明绑定到maven的compile阶段 -->
                            <goals>
                                <goal>compile</goal>
                                <goal>testCompile</goal>
                            </goals>
                        </execution>
                    </executions>
                </plugin>

                <!-- 用于项目的打包插件 -->
                <plugin>
                    <groupId>org.apache.maven.plugins</groupId>
                    <artifactId>maven-assembly-plugin</artifactId>
                    <version>3.0.0</version>
                    <executions>
                        <execution>
                            <id>make-assembly</id>
                            <phase>package</phase>
                            <goals>
                                <goal>single</goal>
                            </goals>
                        </execution>
                    </executions>
                </plugin>
            </plugins>
        </pluginManagement>

    </build>

</project>
```

### recommender的pom文件

```xml
 <dependencyManagement>
        <dependencies>

            <!-- 引入Spark相关的Jar包 -->
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-core_2.11</artifactId>
                <version>2.1.1</version>
                <!-- provider如果存在，那么运行时该Jar包不存在，也不会打包到最终的发布版本中，只是编译器有效 -->
                <!--<scope>provided</scope>-->
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-sql_2.11</artifactId>
                <version>2.1.1</version>
                <!-- provider如果存在，那么运行时该Jar包不存在，也不会打包到最终的发布版本中，只是编译器有效 -->
                <!--<scope>provided</scope>-->
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-streaming_2.11</artifactId>
                <version>2.1.1</version>
                <!-- provider如果存在，那么运行时该Jar包不存在，也不会打包到最终的发布版本中，只是编译器有效 -->
                <!--<scope>provided</scope>-->
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-mllib_2.11</artifactId>
                <version>2.1.1</version>
                <!-- provider如果存在，那么运行时该Jar包不存在，也不会打包到最终的发布版本中，只是编译器有效 -->
                <!--<scope>provided</scope>-->
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-graphx_2.11</artifactId>
                <version>2.1.1</version>
                <!-- provider如果存在，那么运行时该Jar包不存在，也不会打包到最终的发布版本中，只是编译器有效 -->
                <!--<scope>provided</scope>-->
            </dependency>

        </dependencies>
    </dependencyManagement>

    <build>
        <plugins>
            <!-- 如果父项目有声明plugin，那么子项目在引入的时候，不用声明版本和父项目已经声明的配置 -->
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
```

### dataloader的pom文件

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>recommender</artifactId>
        <groupId>com.aishang</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>dataloader</artifactId>
    <properties>
        <mongodb-spark.version>2.0.0</mongodb-spark.version>
        <casbah.version>3.1.1</casbah.version>
        <elasticsearch-spark.version>5.6.2</elasticsearch-spark.version>
        <elasticsearch.version>5.6.2</elasticsearch.version>
    </properties>

    <dependencies>
        <!-- Spark的依赖引入 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
        </dependency>
        <!-- 引入Scala -->
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
        </dependency>


        <!-- 加入MongoDB的驱动 -->
        <!-- 用于代码方式连接MongoDB -->
        <dependency>
            <groupId>org.mongodb</groupId>
            <artifactId>casbah-core_2.11</artifactId>
            <version>${casbah.version}</version>
        </dependency>
        <!-- 用于Spark和MongoDB的对接 -->
        <dependency>
            <groupId>org.mongodb.spark</groupId>
            <artifactId>mongo-spark-connector_2.11</artifactId>
            <version>${mongodb-spark.version}</version>
        </dependency>

        <!-- 加入ElasticSearch的驱动 -->
        <!-- 用于代码方法连接ElasticSearch -->
        <dependency>
            <groupId>org.elasticsearch.client</groupId>
            <artifactId>transport</artifactId>
            <version>${elasticsearch.version}</version>
        </dependency>
        <!-- 用于Spark和ElasticSearch的对接 -->
        <dependency>
            <groupId>org.elasticsearch</groupId>
            <artifactId>elasticsearch-spark-20_2.11</artifactId>
            <version>${elasticsearch-spark.version}</version>
            <!-- 将依赖的包从依赖路径中除去 -->
            <exclusions>
                <exclusion>
                    <groupId>org.apache.hive</groupId>
                    <artifactId>hive-service</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

    </dependencies>

</project>
```

### log4j.properties

```properties
log4j.rootLogger=info, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS}  %5p --- [%50t]  %-80c(line:%5L)  :  %m%n

log4j.appender.R=org.apache.log4j.RollingFileAppender
log4j.appender.R.File=../log/agent.log
log4j.appender.R.MaxFileSize=1024KB
log4j.appender.R.MaxBackupIndex=1

log4j.appender.R.layout=org.apache.log4j.PatternLayout
log4j.appender.R.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS}  %5p --- [%50t]  %-80c(line:%6L)  :  %m%n

#log4j.appender.syslog=org.apache.log4j.net.SyslogAppender
#log4j.appender.syslog=com.c4c.dcos.commons.logger.appender.SyslogAppenderExt
#log4j.appender.syslog.SyslogHost= 192.168.22.237
#log4j.appender.syslog.Threshold=INFO
#log4j.appender.syslog.layout=org.apache.log4j.PatternLayout
#log4j.appender.syslog.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS}  %5p --- [%20t]  %-130c:(line:%4L)  :   %m%n
#demo|FATAL|2014-Jul-03 14:34:34,194|main|com.c4c.logdemo.App:(line:15)|send a log
```

复制csv文件到resources目录下

### 新建scala文件

- com.aishang.dataloader
- 写入数据到mongodb

```scala
package com.aishang.dataloader

import java.net.InetAddress

import com.mongodb.casbah.commons.MongoDBObject
import com.mongodb.casbah.{MongoClient, MongoClientURI}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.elasticsearch.action.admin.indices.create.CreateIndexRequest
import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest
import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsRequest
import org.elasticsearch.common.settings.Settings
import org.elasticsearch.common.transport.InetSocketTransportAddress
import org.elasticsearch.transport.client.PreBuiltTransportClient

/**
  * Movie数据集，数据集字段通过分割
  *
  * 151^                          电影的ID
  * Rob Roy (1995)^               电影的名称
  * In the highlands ....^        电影的描述
  * 139 minutes^                  电影的时长
  * August 26, 1997^              电影的发行日期
  * 1995^                         电影的拍摄日期
  * English ^                     电影的语言
  * Action|Drama|Romance|War ^    电影的类型
  * Liam Neeson|Jessica Lange...  电影的演员
  * Michael Caton-Jones           电影的导演
  *
  * tag1|tag2|tag3|....           电影的Tag
  **/

case class Movie(val mid: Int, val name: String, val descri: String, val timelong: String, val issue: String,
                 val shoot: String, val language: String, val genres: String, val actors: String, val directors: String)

/**
  * Rating数据集，用户对于电影的评分数据集，用，分割
  *
  * 1,           用户的ID
  * 31,          电影的ID
  * 2.5,         用户对于电影的评分
  * 1260759144   用户对于电影评分的时间
  */
case class Rating(val uid: Int, val mid: Int, val score: Double, val timestamp: Int)

/**
  * Tag数据集，用户对于电影的标签数据集，用，分割
  *
  * 15,          用户的ID
  * 1955,        电影的ID
  * dentist,     标签的具体内容
  * 1193435061   用户对于电影打标签的时间
  */
case class Tag(val uid: Int, val mid: Int, val tag: String, val timestamp: Int)

/**
  * MongoDB的连接配置
  * @param uri   MongoDB的连接
  * @param db    MongoDB要操作数据库
  */
case class MongoConfig(val uri:String, val db:String)

/**
  * ElasticSearch的连接配置
  * @param httpHosts       Http的主机列表，以，分割
  * @param transportHosts  Transport主机列表， 以，分割
  * @param index           需要操作的索引
  * @param clustername     ES集群的名称，
  */
case class ESConfig(val httpHosts:String, val transportHosts:String, val index:String, val clustername:String)

// 数据的主加载服务
object DataLoader {

  val MOVIE_DATA_PATH = "/Users/liujiang/Documents/IDEA/MovieSystem/recommender/dataloader/src/main/resources/movies.csv"
  val RATING_DATA_PATH = "/Users/liujiang/Documents/IDEA/MovieSystem/recommender/dataloader/src/main/resources/ratings.csv"
  val TAG_DATA_PATH = "/Users/liujiang/Documents/IDEA/MovieSystem/recommender/dataloader/src/main/resources/tags.csv"

  val MONGODB_MOVIE_COLLECTION = "Movie"
  val MONGODB_RATING_COLLECTION = "Rating"
  val MONGODB_TAG_COLLECTION = "Tag"

  val ES_MOVIE_INDEX = "Movie"

  // 程序的入口
  def main(args: Array[String]): Unit = {

    val config = Map(
      "spark.cores" -> "local[*]",
      "mongo.uri" -> "mongodb://192.168.83.30:27017/recommender",
      "mongo.db" -> "recommender",
      "es.httpHosts" -> "192.168.83.30:9200",
      "es.transportHosts" -> "192.168.83.30:9300",
      "es.index" -> "recommender",
      "es.cluster.name" -> "es-cluster"
    )


    // 需要创建一个SparkConf配置
    val sparkConf = new SparkConf().setAppName("DataLoader").setMaster(config.get("spark.cores").get)

    // 创建一个SparkSession
    val spark = SparkSession.builder().config(sparkConf).getOrCreate()

    import spark.implicits._

    // 将Movie、Rating、Tag数据集加载进来
    val movieRDD = spark.sparkContext.textFile(MOVIE_DATA_PATH)
    //将MovieRDD装换为DataFrame
    val movieDF = movieRDD.map(item =>{
      val attr = item.split("\\^")
      Movie(attr(0).toInt,attr(1).trim,attr(2).trim,attr(3).trim,attr(4).trim,attr(5).trim,attr(6).trim,attr(7).trim,attr(8).trim,attr(9).trim)
    }).toDF()

    val ratingRDD = spark.sparkContext.textFile(RATING_DATA_PATH)
    //将ratingRDD转换为DataFrame
    val ratingDF = ratingRDD.map(item => {
      val attr = item.split(",")
      Rating(attr(0).toInt,attr(1).toInt,attr(2).toDouble,attr(3).toInt)
    }).toDF()

    val tagRDD = spark.sparkContext.textFile(TAG_DATA_PATH)
    //将tagRDD装换为DataFrame
    val tagDF = tagRDD.map(item => {
      val attr = item.split(",")
      Tag(attr(0).toInt,attr(1).toInt,attr(2).trim,attr(3).toInt)
    }).toDF()

    val mongoConfig = MongoConfig(config.get("mongo.uri").get,config.get("mongo.db").get)

    // 需要将数据保存到MongoDB中
    storeDataInMongoDB(movieDF, ratingDF, tagDF,mongoConfig)





    // 关闭Spark
    spark.stop()
  }

  // 将数据保存到MongoDB中的方法
  def storeDataInMongoDB(movieDF: DataFrame, ratingDF:DataFrame, tagDF:DataFrame,mongoConfig: MongoConfig): Unit = {

    //新建一个到MongoDB的连接
    val mongoClient = MongoClient(MongoClientURI(mongoConfig.uri))

    //如果MongoDB中有对应的数据库，那么应该删除
    mongoClient(mongoConfig.db)(MONGODB_MOVIE_COLLECTION).dropCollection()
    mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION).dropCollection()
    mongoClient(mongoConfig.db)(MONGODB_TAG_COLLECTION).dropCollection()

    //将当前数据写入到MongoDB
    movieDF
      .write
      .option("uri",mongoConfig.uri)
      .option("collection",MONGODB_MOVIE_COLLECTION)
      .mode("overwrite")
      .format("com.mongodb.spark.sql")
      .save()

    ratingDF
      .write
      .option("uri",mongoConfig.uri)
      .option("collection",MONGODB_RATING_COLLECTION)
      .mode("overwrite")
      .format("com.mongodb.spark.sql")
      .save()

    tagDF
      .write
      .option("uri",mongoConfig.uri)
      .option("collection",MONGODB_TAG_COLLECTION)
      .mode("overwrite")
      .format("com.mongodb.spark.sql")
      .save()

    //对数据表建索引
    mongoClient(mongoConfig.db)(MONGODB_MOVIE_COLLECTION).createIndex(MongoDBObject("mid" -> 1))
    mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION).createIndex(MongoDBObject("uid" -> 1))
    mongoClient(mongoConfig.db)(MONGODB_RATING_COLLECTION).createIndex(MongoDBObject("mid" -> 1))
    mongoClient(mongoConfig.db)(MONGODB_TAG_COLLECTION).createIndex(MongoDBObject("uid" -> 1))
    mongoClient(mongoConfig.db)(MONGODB_TAG_COLLECTION).createIndex(MongoDBObject("mid" -> 1))

    //关闭MongoDB的连接
    mongoClient.close()
  }



}


```

![_------_--------------------_](/Users/liujiang/Documents/Typora/imgs/006tNc79ly1g658cbla3uj31i00u04bg.jpg)

- 写入数据到es

```scala

```

